# main.py  
import torch  
import numpy as np  
from torch.utils.data import Dataset, DataLoader
import os

from dataloader import PKLDataset
from modules import GroovifyNet, BinarySplitDecoder, hierarchical_interpolate, AdaptiveResampler

from loss import RhythmLoss
#config
tree_depth = 8
n_resampling_bin = 2**tree_depth - 1

class GroovifyOptimizer:  
    def __init__(self, config):  
        self.config = config  
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        dataset = PKLDataset("./fullbaseline_split")
        self.dataloader = DataLoader(
                dataset,
                batch_size=32,
                shuffle=True,
                num_workers=os.cpu_count()//2,
                pin_memory=True,
                prefetch_factor=2,
                persistent_workers=True
            )
        self.resample1 = AdaptiveResampler(256) # resample to 256 frame
        self.resample2 = AdaptiveResampler(150) # resample to 150 frame
        self.model = GroovifyNet().to(self.device)  
        self.binarydecoder = BinarySplitDecoder(tree_depth).to(self.device)
        self.loss_fn = RhythmLoss().to(self.device)
        
        self.optimizer = torch.optim.AdamW(  
            self.model.parameters(),  
            lr=config['learning_rate'],  
            weight_decay=config['weight_decay']  
        )  

    def train_epoch(self, epoch):  
        self.model.train()  
        total_loss = 0
        total_original_loss = 0
        for batch_idx, (bpm, motion_seq) in enumerate(self.dataloader):  
            bpm, motion_seq = bpm.to(self.device), motion_seq.to(self.device)   # Bx1, BxTx147(24*6 + 3)      
             
            # preprocess data (resampling)
            batch_size = motion_seq.shape[0]
            orig_seq_len = motion_seq.shape[1]
            sampled_motion_seq = self.resample1(motion_seq)
            # Z-score normalization with common BPM statistics
            norm_bpm = (bpm - 120.0) / 40.0  # Mean=120, STD=40
            
            self.optimizer.zero_grad()  
            out_activations = self.model(sampled_motion_seq, bpm)
            # if torch.isnan(out_activations).any():
            #     import pdb; pdb.set_trace()
            # # print(f"{out_activations.min()}. {out_activations.max()}")
            binarytree =  self.binarydecoder(out_activations)
            interpolation_seq = hierarchical_interpolate(sampled_motion_seq, binarytree)
            out_motion_seq= self.resample2(interpolation_seq)
            original_loss = self.loss_fn(motion_seq, bpm)
            loss = self.loss_fn(out_motion_seq, bpm)
            loss.backward() 
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)  
            self.optimizer.step()  
            total_loss += loss.item()
            total_original_loss += original_loss.item()
        print(total_original_loss / len(self.dataloader)  )
        return total_loss / len(self.dataloader)  

    # def validate(self, epoch):  
    #     self.model.eval()  
    #     val_loss = 0  
    #     with torch.no_grad():  
    #         for inputs, targets in self.data_loader.val_loader:  
    #             inputs, targets = inputs.to(self.device), targets.to(self.device)  
    #             outputs = self.model(inputs)  
    #             val_loss += self.loss_fn(outputs, targets).item()  
    #     avg_val_loss = val_loss / len(self.data_loader.val_loader)  
    #     self.visualizer.update_val_metrics(epoch, avg_val_loss)  
    #     return avg_val_loss  

    def optimize(self):  
        best_loss = float('inf')  
        for epoch in range(self.config['epochs']):  
            train_loss = self.train_epoch(epoch)  
            # val_loss = self.validate(epoch)  
            print(f"Epoch {epoch+1}/{self.config['epochs']} | "  
                  f"Train Loss: {train_loss:.4f}") # | Val Loss: {val_loss:.4f}")  
            # if val_loss < best_loss:  
            #     best_loss = val_loss  
            #     torch.save(self.model.state_dict(), 'best_model.pth')
            
if __name__ == "__main__":  
    config = {  
        'data_path': './data/',  
        'model_params': {  
            'input_dim': 128,  
            'hidden_dim': 256,  
            'output_dim': 64  
        },  
        'learning_rate': 3e-4,  
        'weight_decay': 1e-5,  
        'epochs': 100,  
        'batch_size': 32  
    }  
    # Hyperparameter search configuration  
    search_space = {   
        'learning_rate': [1e-4, 3e-4, 1e-3],  
        'hidden_dim': [128, 256, 512],  
        'dropout_rate': [0.2, 0.3, 0.4]  
        # + layer depth, high level skip, resampling rate, 
    }  
    optimizer = GroovifyOptimizer(config)  
    optimizer.optimize()  
